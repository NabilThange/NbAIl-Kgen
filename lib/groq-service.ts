import Groq from 'groq-sdk';

const groq = new Groq({
  apiKey: process.env.NEXT_PUBLIC_GROQ_API_KEY,
  dangerouslyAllowBrowser: true,
});

export const getGroqChatCompletion = async (userMessage: string) => {
  const systemPrompt = `> You are a **conversational AI** with advanced contextual understanding, emotional adaptability, and subject-mastery across technical, creative, academic, and real-world domains. You respond to queries like an expert human assistant, maintaining the following principles:

---

### ðŸŽ¯ OBJECTIVES

1.  **Context Awareness**
    *   Always keep track of ongoing conversations.
    *   Build your answers based on previous messages and adjust tone accordingly.
    *   Avoid repeating context the user already knows unless clarification is needed.

2.  **Tone & Personality Adaptation**
    *   Match the user's **tone**, **mood**, and **style**:
        *   Friendly and casual when the user is relaxed.
        *   Professional and precise when the user is formal.
        *   Empathetic when the topic is emotional or sensitive.
    *   Use contractions, rhetorical flow, and nuanced expressions when appropriate.

3.  **Structured Responses**
    *   Organize information clearly with titles, bullets, steps, or summaries.
    *   Use Markdown-like styling (bold titles, italics, emojis if casual).
    *   Avoid walls of text â€” break information into digestible parts.

4.  **Precision + Depth**
    *   Ensure your answers are accurate, well-researched, and cite best practices when needed.
    *   Use analogies, examples, and comparisons to help explain complex ideas simply.
    *   Never make up facts â€” say "I don't know" if needed, or suggest what to look for.

5.  **Creative & Logical Balance**
    *   Capable of both **creative tasks** (writing poetry, branding, design ideas) and **logical tasks** (math, code, research).
    *   Know when to be imaginative and when to be factual.

6.  **Multimodal Flexibility** *(if supported)*
    *   Capable of understanding and describing text, images, charts, diagrams, or even audio/video.
    *   Answer accordingly when files are uploaded or referenced.

7.  **Memory & Continuity**
    *   If given access, remember key facts and preferences shared by the user throughout the session or across conversations.

---

### ðŸ§  EXAMPLES OF BEHAVIOR:

*   **Coding help?** Give step-by-step logic first, then show clean code, with optional explanations.
*   **Design request?** Start with principles (contrast, hierarchy), then offer suggestions or mock copy/design.
*   **Confused user?** Slow down, rephrase simply, check if they understand before moving on.
*   **In a rush?** Give a summary first, then offer details if the user asks.
*   **Technical document?** Use formal language, inline definitions, and modular explanations.

---

### ðŸ§¬ Your Role:

> You are not just answering â€” you are **collaborating**, **adapting**, and **enhancing** how the user thinks and works.

---

Always format your responses using clear Markdown syntax (bold, italics, lists, headings, code blocks) for structure and readability.`;

  try {
    const chatCompletion = await groq.chat.completions.create({
      messages: [
        {
          role: 'system',
          content: systemPrompt,
        },
        {
          role: 'user',
          content: userMessage,
        },
      ],
      model: 'llama3-8b-8192', // Or your preferred model
    });
    return chatCompletion.choices[0]?.message?.content || 'Sorry, I could not generate a response.';
  } catch (error) {
    console.error('Error getting Groq chat completion:', error);
    return 'Sorry, there was an error processing your request.';
  }
};

export const getGroqTranscription = async (audioFile: File) => {
  try {
    const transcription = await groq.audio.transcriptions.create({
      file: audioFile,
      model: "whisper-large-v3", // Or your preferred model
      response_format: "json", // Get transcription text
    });
    return transcription.text || ""; // Return the transcribed text
  } catch (error) {
    console.error("Error getting Groq transcription:", error);
    return "Sorry, there was an error transcribing the audio.";
  }
};

export const getGroqVisionCompletion = async (userPrompt: string, imageBase64: string, imageType: string) => {
  // Ensure the base64 string has the correct prefix (e.g., data:image/jpeg;base64,)
  // The FileReader already provides this, but double-check or construct if needed.
  const imageUrl = imageBase64.startsWith('data:') ? imageBase64 : `data:${imageType};base64,${imageBase64}`;

  try {
    console.log("Sending to Groq Vision..."); // Debug log
    const chatCompletion = await groq.chat.completions.create({
      messages: [
        {
          role: "user",
          content: [
            {
              type: "text",
              text: userPrompt,
            },
            {
              type: "image_url",
              image_url: {
                "url": imageUrl,
              },
            },
          ],
        },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct", // Use the specified Llama 4 Scout vision model
      max_tokens: 1024, // Example limit, adjust as needed
    });
    console.log("Received from Groq Vision:", chatCompletion.choices[0]?.message?.content);
    return chatCompletion.choices[0]?.message?.content || "Sorry, I could not process the image.";
  } catch (error) {
    console.error("Error getting Groq vision completion:", error);
    // Check for specific error types if needed (e.g., size limits)
    if (error instanceof Error && error.message.includes('413')) {
        return "Sorry, the uploaded image is too large.";
    }
    return "Sorry, there was an error processing the image.";
  }
};

// Specific function for AR mode or general vision queries
export const getGroqVisionAnalysis = async (
  base64ImageData: string, // Expecting data URL like "data:image/jpeg;base64,..."
  prompt: string,
) => {
  if (!process.env.NEXT_PUBLIC_GROQ_API_KEY) {
    throw new Error("Groq API key not configured.");
  }
  if (!base64ImageData || !base64ImageData.startsWith("data:image")) {
      throw new Error("Invalid image data provided.");
  }

  // Extract base64 content and determine MIME type
  const mimeType = base64ImageData.substring(base64ImageData.indexOf(":") + 1, base64ImageData.indexOf(";"));
  const base64Content = base64ImageData.substring(base64ImageData.indexOf(",") + 1);

  console.log(`Sending to Groq Vision: Prompt - "${prompt}", MimeType - ${mimeType}, Size approx ${Math.round(base64Content.length * 3/4 / 1024)} KB`);

  try {
    const completion = await groq.chat.completions.create({
      messages: [
        {
          role: "system",
          content: "You are a vision-enabled AI assistant built for AR Mode. Analyze the provided image in the context of the user's query. Respond clearly and concisely, highlighting the most relevant visual details. Prioritize accuracy and usefulness.",
        },
        {
          role: "user",
          content: [
            {
              type: "image_url",
              image_url: {
                url: base64ImageData, // Send the full data URL
              },
            },
            {
              type: "text",
              text: prompt,
            },
          ],
        },
      ],
      model: "meta-llama/llama-4-scout-17b-16e-instruct",
      // Optional: Add parameters like max_tokens, temperature if needed
      // max_tokens: 150,
    });

    const responseContent = completion.choices[0]?.message?.content;
    console.log("Groq Vision API Response:", responseContent);

    if (!responseContent) {
      throw new Error("Received an empty response from Groq Vision API.");
    }

    return responseContent;

  } catch (error) {
    console.error("Error calling Groq Vision API:", error);
    // Rethrow or handle appropriately
    if (error instanceof Error) {
       throw new Error(`Groq Vision API Error: ${error.message}`);
    } else {
       throw new Error("An unknown error occurred while contacting Groq Vision API.");
    }
  }
}; 